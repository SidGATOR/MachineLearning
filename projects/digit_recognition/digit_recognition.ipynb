{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Digit Recognition Program\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Design and Test a Model Architecture\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "You can use ** Keras ** to implement your model. Read more at [keras.io](https://keras.io/).\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0)). You are not expected to model your architecture precisely using this model nor get the same performance levels, but this is more to show an exampe of an approach used to solve this particular problem. We encourage you to try out different architectures for yourself and see what works best for you. Here is a useful [forum post](https://discussions.udacity.com/t/goodfellow-et-al-2013-architecture/202363) discussing the architecture as described in the paper and here is [another one](https://discussions.udacity.com/t/what-loss-function-to-use-for-multi-digit-svhn-training/176897) discussing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "filepath_mnist = './models/mnist/checkpoint'\n",
    "mnist = input_data.read_data_sets('.',one_hot=True,reshape=False)\n",
    "\n",
    "# Save File #\n",
    "save_file = \"./models/mnist/save_test_model\"\n",
    "\n",
    "## Input Parameters ##\n",
    "n_classes = 10\n",
    "last_loss = None\n",
    "\n",
    "## Model Hyperparameters ##\n",
    "epochs =20000\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.5\n",
    "test_size = 256\n",
    "batch_size = 50\n",
    "epsilon = 1e-3\n",
    "decay = 0.999\n",
    "\n",
    "## Definition ##\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "\n",
    "    image_shape = [None,image_shape[0],image_shape[1],image_shape[2]]\n",
    "    X = tf.placeholder(tf.float32,shape=image_shape,name=\"x\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "\n",
    "    Y = tf.placeholder(tf.float32,[None,n_classes],name=\"y\")\n",
    "    return Y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "    return keep_prob\n",
    "\n",
    "def batch_norm_wrapper(x_tensor,is_training):\n",
    "\n",
    "    ## Tensor Shape ##\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    ## Gamma and Beta ##\n",
    "    gamma = tf.Variable(tf.ones(shape=[tensor_shape[-1]]))\n",
    "    beta = tf.Variable(tf.zeros(shape=[tensor_shape[-1]]))\n",
    "    pop_mean = tf.Variable(tf.zeros(shape=[tensor_shape[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones(shape=[tensor_shape[-1]]), trainable=False)\n",
    "\n",
    "    if is_training:\n",
    "        if len(tensor_shape) == 2:\n",
    "            batch_mean, batch_var = tf.nn.moments(x_tensor,axes=[0])\n",
    "        elif len(tensor_shape) == 4:\n",
    "            batch_mean, batch_var = tf.nn.moments(x_tensor,axes=[0,1,2])\n",
    "        else:\n",
    "            print(\"Wrong Dimensions\")\n",
    "            exit()\n",
    "\n",
    "        train_mean = tf.assign(pop_mean,pop_mean * decay + (1 - decay) * batch_mean)\n",
    "        train_var = tf.assign(pop_var,pop_var * decay + (1 - decay) * batch_var)\n",
    "\n",
    "        with tf.control_dependencies([train_mean,train_var]):\n",
    "            return tf.nn.batch_normalization(x_tensor,batch_mean,batch_var,beta,gamma,epsilon)\n",
    "    else:\n",
    "        return tf.nn.batch_normalization(x_tensor,pop_mean,pop_var,beta,gamma,epsilon)\n",
    "\n",
    "\n",
    "def xavier_init(x_tensor,weight_dim):\n",
    "\n",
    "    Nin = x_tensor.get_shape().as_list()[-1]\n",
    "    Nout = weight_dim[-1]\n",
    "\n",
    "    return tf.cast(tf.sqrt(tf.divide(2,tf.add(Nin,Nout))),tf.float32)\n",
    "\n",
    "\n",
    "def conv2d_maxpool(x_tensor,conv_num_outputs,conv_ksize,conv_stride,pool_ksize,pool_stride,is_training):\n",
    "\n",
    "    ## Tensor Shape ##\n",
    "    # [batch,height,width,depth]\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "\n",
    "    ## Weight and Bias Dimensions #\n",
    "    weight_dim = [*conv_ksize,tensor_shape[-1],conv_num_outputs]\n",
    "    bias_dim = [conv_num_outputs]\n",
    "\n",
    "    ## Filter Dimensions ##\n",
    "    filter_stride = [1,*conv_stride,1]\n",
    "\n",
    "    ## Pooling Dimensions ##\n",
    "    pool_stride = [1,*pool_stride,1]\n",
    "    pool_ksize = [1,*pool_ksize,1]\n",
    "\n",
    "    ## Weights and Biases ##\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=weight_dim,stddev=xavier_init(x_tensor,weight_dim)))\n",
    "    biases = tf.Variable(tf.constant(0.1,shape=bias_dim))\n",
    "\n",
    "    ## Convolution ##\n",
    "    conv_layer = tf.nn.bias_add(tf.nn.conv2d(x_tensor,weights,filter_stride,padding=\"SAME\"),biases)\n",
    "\n",
    "    ## Batch Normalization ##\n",
    "    conv_layer = batch_norm_wrapper(conv_layer,is_training)\n",
    "\n",
    "    ## Activation ##\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    ## Max Pooling ##\n",
    "    conv_layer = tf.nn.max_pool(conv_layer,pool_ksize,pool_stride,padding=\"SAME\")\n",
    "\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "## Change tensor from 4D to 2D for dense layers\n",
    "def flatten(x_tensor):\n",
    "\n",
    "    ## Tensor shape ##\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    tensor_shape = tensor_shape[1] * tensor_shape[2] * tensor_shape[3]\n",
    "\n",
    "    return tf.reshape(x_tensor,shape=[-1,tensor_shape])\n",
    "\n",
    "def fully_conn(x_tensor,num_outputs,is_training):\n",
    "\n",
    "    ## Tensor shape ##\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "\n",
    "    ## Weight and Bias Dimensions ##\n",
    "    weight_dim = [tensor_shape[-1],num_outputs]\n",
    "    bias_dim = [num_outputs]\n",
    "\n",
    "    ##Weights and Biases\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=weight_dim,stddev=xavier_init(x_tensor,weight_dim)))\n",
    "    biases = tf.Variable(tf.constant(0.1,shape=bias_dim))\n",
    "\n",
    "    ## Forward Propogation ##\n",
    "    fc1 = tf.add(tf.matmul(x_tensor,weights),biases)\n",
    "\n",
    "    ## Batch Normalization ##\n",
    "    fc1 = batch_norm_wrapper(x_tensor,is_training)\n",
    "\n",
    "    ## Activation ##\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    return fc1\n",
    "\n",
    "\n",
    "def output(x_tensor,num_outputs):\n",
    "\n",
    "    ## Weight and Bias Dimensions ##\n",
    "    weight_dim = [x_tensor.get_shape().as_list()[-1],num_outputs]\n",
    "    bias_dim = [num_outputs]\n",
    "\n",
    "    ## Weights and Biases ##\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=weight_dim,stddev=xavier_init(x_tensor,weight_dim)))\n",
    "    biases = tf.Variable(tf.constant(0.1,shape=bias_dim))\n",
    "\n",
    "    ## Forward Propogation ##\n",
    "    out = tf.add(tf.matmul(x_tensor,weights),biases,name=\"output\")\n",
    "\n",
    "    return out\n",
    "\n",
    "def conv_net(is_training,keep_prob):\n",
    "\n",
    "    ## Features and Labels ##\n",
    "    features = neural_net_image_input((28,28,1))\n",
    "    labels = neural_net_label_input((10))\n",
    "\n",
    "\n",
    "    ## Convolutions layer parameters ## \n",
    "    conv_param = {\"conv1_num_outputs\" : 32, \"conv1_conv_ksize\" : (5,5), \"conv1_conv_strides\" : (1,1),\n",
    "                  \"conv1_pool_ksize\" : (2,2), \"conv1_pool_strides\" : (2,2), \"conv2_num_outputs\" : 64,\n",
    "                  \"conv2_conv_ksize\" : (5,5), \"conv2_conv_strides\" : (1,1), \"conv2_pool_ksize\" : (2,2),\n",
    "                  \"conv2_pool_strides\" : (2,2), \"conv3_num_outputs\" : 128 , \"conv3_conv_ksize\" : (3,3),\n",
    "                  \"conv3_conv_strides\" : (1,1), \"conv3_pool_ksize\" : (2,2),\"conv3_pool_strides\" : (2,2) }\n",
    "\n",
    "    ## Parameters: Fully Connected Layer ##\n",
    "    ## Current best fc_param = {\"fc1_num_outputs\" : 1024 , \"fc2_num_outputs\" : 256,\"fc3_num_outputs\" : 64, \"dropout\": keep_prob }\n",
    "    fc_param = {\"fc1_num_outputs\" : 1024 , \"fc2_num_outputs\" : 64,\"fc3_num_outputs\" : 30, \"dropout\": keep_prob }\n",
    "\n",
    "    ## Paramaters: Output layer ##\n",
    "    output_param = {\"output_num_outputs\" : 10}\n",
    "\n",
    "\n",
    "    ## Layer 1 ##\n",
    "    conv1_layer = conv2d_maxpool(features,conv_param[\"conv1_num_outputs\"], conv_param[\"conv1_conv_ksize\"], conv_param[\"conv1_conv_strides\"],\n",
    "                                 conv_param[\"conv1_pool_ksize\"],conv_param[\"conv1_pool_strides\"],is_training)\n",
    "\n",
    "    ## Layer 2 ##\n",
    "    conv2_layer = conv2d_maxpool(conv1_layer,conv_param[\"conv2_num_outputs\"], conv_param[\"conv2_conv_ksize\"], conv_param[\"conv2_conv_strides\"],\n",
    "                                 conv_param[\"conv2_pool_ksize\"],conv_param[\"conv2_pool_strides\"],is_training)\n",
    "    # Layer 3 ##\n",
    "    conv3_layer = conv2d_maxpool(conv2_layer,conv_param[\"conv3_num_outputs\"], conv_param[\"conv3_conv_ksize\"], conv_param[\"conv3_conv_strides\"],\n",
    "                                 conv_param[\"conv3_pool_ksize\"],conv_param[\"conv3_pool_strides\"],is_training)\n",
    "\n",
    "    ## Flattening ##\n",
    "    # Convert from 4D to 2D\n",
    "    flat = flatten(conv3_layer)\n",
    "\n",
    "    ## Fully Connected Layer 1 ##\n",
    "    fc1_layer = fully_conn(flat,fc_param[\"fc1_num_outputs\"],is_training)\n",
    "    # Dropout #\n",
    "    fc1_layer = tf.nn.dropout(fc1_layer,keep_prob=keep_prob)\n",
    "    \n",
    "    ## Fully Connected Layer 2 ##\n",
    "    fc2_layer = fully_conn(fc1_layer,fc_param[\"fc1_num_outputs\"],is_training)\n",
    "    # Dropout #\n",
    "    fc2_layer = tf.nn.dropout(fc2_layer,keep_prob=keep_prob)\n",
    "    \n",
    "    ## Output Layer ##\n",
    "    out_layer = output(fc2_layer,output_param[\"output_num_outputs\"])\n",
    "\n",
    "    ## Cost or Cross Entropy Loss ##\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=labels))\n",
    "\n",
    "    ## Optimizer ##\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    ## Predictions ##\n",
    "    predictions = tf.equal(tf.argmax(out_layer,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions,tf.float32), name=\"accuracy\")\n",
    "\n",
    "    return (features,labels), optimizer, cost, accuracy, out_layer, tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_test_model():\n",
    "    ## Dropout Probability ##\n",
    "    tf.reset_default_graph()\n",
    "    keep_probability = neural_net_keep_prob_input()\n",
    "    (features,labels_), optimizer, cost, accuracy, _, saver = conv_net(True,keep_probability)\n",
    "\n",
    "    acc = []\n",
    "    last_loss = None\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        total_batches = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(epochs)):\n",
    "\n",
    "            #for i in range(total_batches):\n",
    "\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            ## Run Optimizer ##\n",
    "            sess.run(optimizer,feed_dict={features : batch_x, labels_ : batch_y, keep_probability : dropout})\n",
    "\n",
    "            if epoch % (epochs / 10) == 0:\n",
    "                loss = sess.run(cost,feed_dict={features : batch_x, labels_ : batch_y, keep_probability : 1.0})\n",
    "                vali_acc = sess.run(accuracy, feed_dict={features : mnist.validation.images, labels_ : mnist.validation.labels, keep_probability : 1.0})\n",
    "                #acc.append(vali_acc)\n",
    "                if last_loss and last_loss > loss:\n",
    "                    saver.save(sess,save_file)\n",
    "                else:\n",
    "                    print(\"Validation loss has not decreased\")\n",
    "                last_loss = loss\n",
    "                print(\"Epoch #: {:}, Loss: {:}, Validation Accuracy: {:} \" .format(epoch+1,loss,vali_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_model(new_paths):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    keep_probability = neural_net_keep_prob_input()\n",
    "    (features,labels_), _, _, accuracy, out_layer, saver = conv_net(False,keep_probability)\n",
    "    save_file = new_paths[1]\n",
    "    predictions = []\n",
    "    correct = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, save_file)\n",
    "        for i in range(100):\n",
    "            pred, corr = sess.run([tf.argmax(out_layer,1), accuracy],\n",
    "                                 feed_dict={features: [mnist.test.images[i]], labels_: [mnist.test.labels[i]], keep_probability : 1.0})\n",
    "            correct += corr\n",
    "            predictions.append(pred[0])\n",
    "    print(\"PREDICTIONS:\", predictions)\n",
    "    print(\"ACCURACY:\", correct/100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saver_loader_start(filepath):\n",
    "    if not isfile(filepath):\n",
    "        save_test_model()\n",
    "    paths = filepath.split(\"/\")\n",
    "    paths = paths[:-1]\n",
    "    files = [\"/save_test_model.meta\",\"/save_test_model\"]\n",
    "    new_paths = []\n",
    "    for file in files:\n",
    "        new_paths.append('/'.join(paths) + file)\n",
    "    load_test_model(new_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/mnist/save_test_model\n",
      "PREDICTIONS: [7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 8, 4, 9, 6, 6, 5, 4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2, 4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0, 2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 4, 3, 1, 4, 1, 7, 6, 9]\n",
      "ACCURACY: 0.98\n"
     ]
    }
   ],
   "source": [
    "saver_loader_start(filepath_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Train a Model on a Realistic Dataset\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tqdm\n",
    "import helper\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os.path import isfile,abspath\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Preprocessed data....\n"
     ]
    }
   ],
   "source": [
    "filepath_svhn = './models/svhn/checkpoint'\n",
    "features_train, labels_train, features_test, labels_test = helper.preprocess(abspath('./'))\n",
    "features_train,features_validate, labels_train, labels_validate = train_test_split(features_train,labels_train,test_size=0.25,random_state=42)\n",
    "\n",
    "# Save File #\n",
    "save_file = \"./models/svhn/save_streetview_model\"\n",
    "\n",
    "## Input Parameters ##\n",
    "n_classes = 10\n",
    "last_loss = None\n",
    "\n",
    "## Model Hyperparameters ##\n",
    "epochs = 10000\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.7\n",
    "test_size = 256\n",
    "batch_size = 50\n",
    "epsilon = 1e-3\n",
    "decay = 0.999\n",
    "\n",
    "\n",
    "\n",
    "## Definition ##\n",
    "def conv_net_svhn(is_training,keep_prob):\n",
    "\n",
    "    ## Features and Labels ##\n",
    "    features = neural_net_image_input((32,32,1))\n",
    "    labels = neural_net_label_input((10))\n",
    "\n",
    "\n",
    "    ## Convolutions layer parameters ## \n",
    "    conv_param = {\"conv1_num_outputs\" : 32, \"conv1_conv_ksize\" : (5,5), \"conv1_conv_strides\" : (1,1),\n",
    "                  \"conv1_pool_ksize\" : (2,2), \"conv1_pool_strides\" : (2,2), \"conv2_num_outputs\" : 64,\n",
    "                  \"conv2_conv_ksize\" : (5,5), \"conv2_conv_strides\" : (1,1), \"conv2_pool_ksize\" : (2,2),\n",
    "                  \"conv2_pool_strides\" : (2,2), \"conv3_num_outputs\" : 128 , \"conv3_conv_ksize\" : (3,3),\n",
    "                  \"conv3_conv_strides\" : (1,1), \"conv3_pool_ksize\" : (2,2),\"conv3_pool_strides\" : (2,2) }\n",
    "\n",
    "    ## Parameters: Fully Connected Layer ##\n",
    "    ## Current best fc_param = {\"fc1_num_outputs\" : 1024 , \"fc2_num_outputs\" : 256,\"fc3_num_outputs\" : 64, \"dropout\": keep_prob }\n",
    "    fc_param = {\"fc1_num_outputs\" : 1024 , \"fc2_num_outputs\" : 64,\"fc3_num_outputs\" : 30, \"dropout\": keep_prob }\n",
    "\n",
    "    ## Paramaters: Output layer ##\n",
    "    output_param = {\"output_num_outputs\" : 10}\n",
    "\n",
    "\n",
    "    ## Layer 1 ##\n",
    "    conv1_layer = conv2d_maxpool(features,conv_param[\"conv1_num_outputs\"], conv_param[\"conv1_conv_ksize\"], conv_param[\"conv1_conv_strides\"],\n",
    "                                 conv_param[\"conv1_pool_ksize\"],conv_param[\"conv1_pool_strides\"],is_training)\n",
    "\n",
    "    ## Layer 2 ##\n",
    "    conv2_layer = conv2d_maxpool(conv1_layer,conv_param[\"conv2_num_outputs\"], conv_param[\"conv2_conv_ksize\"], conv_param[\"conv2_conv_strides\"],\n",
    "                                 conv_param[\"conv2_pool_ksize\"],conv_param[\"conv2_pool_strides\"],is_training)\n",
    "    # Layer 3 ##\n",
    "    conv3_layer = conv2d_maxpool(conv2_layer,conv_param[\"conv3_num_outputs\"], conv_param[\"conv3_conv_ksize\"], conv_param[\"conv3_conv_strides\"],\n",
    "                                 conv_param[\"conv3_pool_ksize\"],conv_param[\"conv3_pool_strides\"],is_training)\n",
    "\n",
    "    ## Flattening ##\n",
    "    # Convert from 4D to 2D\n",
    "    flat = flatten(conv3_layer)\n",
    "\n",
    "    ## Fully Connected Layer 1 ##\n",
    "    fc1_layer = fully_conn(flat,fc_param[\"fc1_num_outputs\"],is_training)\n",
    "    # Dropout #\n",
    "    fc1_layer = tf.nn.dropout(fc1_layer,keep_prob=keep_prob)\n",
    "    \n",
    "    ## Fully Connected Layer 2 ##\n",
    "    fc2_layer = fully_conn(fc1_layer,fc_param[\"fc1_num_outputs\"],is_training)\n",
    "    # Dropout #\n",
    "    fc2_layer = tf.nn.dropout(fc2_layer,keep_prob=keep_prob)\n",
    "    \n",
    "    ## Output Layer ##\n",
    "    out_layer = output(fc2_layer,output_param[\"output_num_outputs\"])\n",
    "\n",
    "    ## Cost or Cross Entropy Loss ##\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=labels))\n",
    "\n",
    "    ## Optimizer ##\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "    ## Predictions ##\n",
    "    predictions = tf.equal(tf.argmax(out_layer,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(predictions,tf.float32), name=\"accuracy\")\n",
    "\n",
    "    return (features,labels), optimizer, cost, accuracy, out_layer, tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_test_model_svhn():\n",
    "    ## Dropout Probability ##\n",
    "    tf.reset_default_graph()\n",
    "    keep_probability = neural_net_keep_prob_input()\n",
    "    (features,labels_), optimizer, cost, accuracy, _, saver = conv_net(True,keep_probability)\n",
    "\n",
    "    acc = []\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(epochs)):\n",
    "            total_batches = int(features_train.shape[0] // batch_size)\n",
    "            #for batch_x, batch_y in helper.batch_features_labels(features_train,labels_train,batch_size):\n",
    "            batch_x,batch_y = helper.batch_features_labels(features_train,labels_train,batch_size)[epoch % total_batches]\n",
    "            ## Run Optimizer ##\n",
    "            sess.run(optimizer,feed_dict={features : batch_x, labels_ : batch_y, keep_probability : dropout})\n",
    "\n",
    "            if epoch % (epochs / 10) == 0:\n",
    "                loss = sess.run(cost,feed_dict={features : features_validate, labels_ : labels_validate, keep_probability : 1.0})\n",
    "                vali_acc = sess.run(accuracy, feed_dict={features : features_validate, labels_ : labels_validate, keep_probability : 1.0})\n",
    "                #acc.append(vali_acc)\n",
    "                if last_loss and last_loss > loss:\n",
    "                    saver.save(sess,save_file)\n",
    "                else:\n",
    "                    print(\"Validation loss has not decreased\")\n",
    "                last_loss = loss\n",
    "                print(\"Epoch #: {:}, Loss: {:}, Validation Accuracy: {:} \" .format(epoch+1,loss,vali_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_model_svhn(new_paths):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    keep_probability = neural_net_keep_prob_input()\n",
    "    (features,labels_), _, _, accuracy, out_layer, saver = conv_net_svhn(False,keep_probability)\n",
    "    save_file = new_paths[1]\n",
    "    predictions = []\n",
    "    correct = 0\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, save_file)\n",
    "        test_acc = sess.run(accuracy, feed_dict = {features : features_test[:5000], labels_ : labels_test[:5000], keep_probability : 1.0})\n",
    "        print(\"Test Accuracy: {:}%\".format(test_acc*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saver_loader_start_svhn(filepath):\n",
    "    if not isfile(filepath):\n",
    "        save_test_model_svhn()\n",
    "    paths = filepath.split(\"/\")\n",
    "    paths = paths[:-1]\n",
    "    files = [\"/save_streetview_model.meta\",\"/save_streetview_model\"]\n",
    "    new_paths = []\n",
    "    for file in files:\n",
    "        new_paths.append('/'.join(paths) + file)\n",
    "    load_test_model_svhn(new_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/svhn/save_streetview_model\n",
      "Test Accuracy: 95.4800009727478%\n"
     ]
    }
   ],
   "source": [
    "saver_loader_start_svhn(filepath_svhn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
